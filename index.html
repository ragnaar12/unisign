<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>UniSign - Traduction Langue des Signes</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f9;
      text-align: center;
      padding: 20px;
    }
    h1 {
      color: #4CAF50;
    }
    .container {
      margin-top: 30px;
    }
    .sign-message {
      font-size: 24px;
      color: #555;
    }
    .video-container {
      position: relative;
      display: inline-block;
      margin-top: 20px;
    }
    video {
      width: 100%;
      height: auto;
      border: 2px solid #4CAF50;
    }
  </style>
</head>
<body>
  <h1>üñêÔ∏è UniSign - Traduction en Langue des Signes</h1>
  <div class="container">
    <div class="video-container">
      <video id="videoElement" autoplay playsinline></video>
    </div>
    <div id="signMessage" class="sign-message">üëã Aucune main d√©tect√©e</div>
  </div>

  <script>
    const videoElement = document.getElementById('videoElement');
    const signMessage = document.getElementById('signMessage');

    // Demander l'acc√®s √† la cam√©ra
    navigator.mediaDevices.getUserMedia({ video: true })
      .then(stream => {
        videoElement.srcObject = stream;
      })
      .catch(err => {
        console.error('Erreur d\'acc√®s √† la cam√©ra:', err);
      });

    // Logique pour traiter les gestes en fonction des doigts lev√©s
    // Cette partie n√©cessiterait l'int√©gration avec un mod√®le ou une biblioth√®que Python (comme MediaPipe) pour la d√©tection des signes

    // Exemple de gestion de gestes (cela peut √™tre reli√© √† votre backend Python pour traiter les images)
    function updateSignMessage(fingerCount) {
      switch (fingerCount) {
        case 0:
          signMessage.textContent = "‚úä Signification: A";
          break;
        case 1:
          signMessage.textContent = "‚òùÔ∏è Signification: D";
          break;
        case 2:
          signMessage.textContent = "‚úåÔ∏è Signification: V";
          break;
        case 5:
          signMessage.textContent = "üñêÔ∏è Signification: Salut";
          break;
        default:
          signMessage.textContent = `ü§ü Nombre de doigts lev√©s: ${fingerCount}`;
      }
    }

    // Exemple de test avec un nombre de doigts fictif (√† remplacer par la logique de reconnaissance dans le backend)
    setTimeout(() => {
      updateSignMessage(5); // Simule la d√©tection d'une main ouverte
    }, 3000);
  </script>

  <!-- Backend Streamlit et d√©tection via MediaPipe (app.py) -->

  <script src="https://cdn.jsdelivr.net/npm/streamlit@0.82.0/streamlit.min.js"></script>
  <script>
    // Streamlit et gestion de la d√©tection d'images vid√©o
    async function streamlitConnect() {
      const response = await fetch('/detect-hands', {
        method: 'POST',
        body: JSON.stringify({
          video_frame: videoElement.srcObject,
        }),
        headers: {
          'Content-Type': 'application/json',
        }
      });
      const data = await response.json();
      updateSignMessage(data.finger_count);
    }
  </script>
</body>
</html>

